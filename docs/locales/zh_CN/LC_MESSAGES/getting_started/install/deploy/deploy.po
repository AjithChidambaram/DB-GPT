# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-26 11:34+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy/deploy.md:1
#: c9d9195862204bb9b526d728b1527a98
msgid "Installation From Source"
msgstr "源码安装"

#: ../../getting_started/install/deploy/deploy.md:3
#: e462f24ec27645c3afd23866fdeea761
msgid ""
"This tutorial gives you a quick walkthrough about use DB-GPT with you "
"environment and data."
msgstr "本教程为您提供了关于如何使用DB-GPT的使用指南。"

#: ../../getting_started/install/deploy/deploy.md:5
#: 065a4cf91565437cbad46726e5aee89c
msgid "Installation"
msgstr "安装"

#: ../../getting_started/install/deploy/deploy.md:7
#: 07ebe19dbb5040419c6016258d975904
msgid "To get started, install DB-GPT with the following steps."
msgstr "请按照以下步骤安装DB-GPT"

#: ../../getting_started/install/deploy/deploy.md:9
#: 1a552a8bb4fe481ba7695e1a2f8985f8
msgid "1. Hardware Requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:10
#: bd292acafdb74b99a570c4a8e126df5d
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements."
msgstr "DB-GPT可以部署在对硬件要求不高的服务器，也可以部署在对硬件要求高的服务器"

#: ../../getting_started/install/deploy/deploy.md:12
#: 913c8d0630f2460997fb856b81967903
#, fuzzy
msgid "Low hardware requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:13
#: 70ca521385c642049789e14ab61bc46b
msgid ""
"The low hardware requirements mode is suitable for integrating with "
"third-party LLM services' APIs, such as OpenAI, Tongyi, Wenxin, or "
"Llama.cpp."
msgstr "Low hardware requirements模式适用于对接第三方模型服务的api,比如OpenAI, 通义千问, 文心.cpp。"

#: ../../getting_started/install/deploy/deploy.md:15
#: 7aacaa2505c447bfa3d0ef6418ae73d2
msgid "DB-GPT provides set proxy api to support LLM api."
msgstr "DB-GPT可以通过设置proxy api来支持第三方大模型服务"

#: ../../getting_started/install/deploy/deploy.md:17
#: 6b5a9f7d61d54a559363a9a5d270a580
msgid "As our project has the ability to achieve ChatGPT performance of over 85%,"
msgstr "由于我们的项目有能力达到85%以上的ChatGPT性能"

#: ../../getting_started/install/deploy/deploy.md:19
#: cf4b1f1115c041cbafb15af61946378c
#, fuzzy
msgid "High hardware requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:20
#: dae1e9a698144a919dc3740cd676eb81
#, fuzzy
msgid ""
"The high hardware requirements mode is suitable for independently "
"deploying LLM services, such as Llama series models, Baichuan, ChatGLM, "
"Vicuna, and other private LLM service. there are certain hardware "
"requirements. However, overall, the project can be deployed and used on "
"consumer-grade graphics cards. The specific hardware requirements for "
"deployment are as follows:"
msgstr ""
"High hardware requirements模式适用于需要独立部署私有大模型服务，比如Llama系列模型，Baichuan, "
"chatglm，vicuna等私有大模型所以对硬件有一定的要求。但总体来说，我们在消费级的显卡上即可完成项目的部署使用，具体部署的硬件说明如下:"

#: ../../getting_started/install/deploy/deploy.md
#: a6457364eccd49c99dc6a020a9aa5185
msgid "GPU"
msgstr "GPU"

#: ../../getting_started/install/deploy/deploy.md
#: 2d737c43c9fd45efbaf1e4204227ab51 bc0aa12f56dd4d26af74d7c10187fc0c
msgid "VRAM Size"
msgstr "显存"

#: ../../getting_started/install/deploy/deploy.md
#: 8bfd4e58a63a42858b6be2d0ce11b2fa
msgid "Performance"
msgstr "Performance"

#: ../../getting_started/install/deploy/deploy.md
#: 9ff11248bfdc43e18e6c29ed95d4f807
msgid "RTX 4090"
msgstr "RTX 4090"

#: ../../getting_started/install/deploy/deploy.md
#: 229e141d0a0e4d558b11c204654f36a9 94a76fe065164a67883a79f75d10139c
msgid "24 GB"
msgstr "24 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 2edf99edf84e43ea8a5dce2a5ad0056a
msgid "Smooth conversation inference"
msgstr "丝滑的对话体验"

#: ../../getting_started/install/deploy/deploy.md
#: df67002e7eb84939a1f452acc88a1fa2
msgid "RTX 3090"
msgstr "RTX 3090"

#: ../../getting_started/install/deploy/deploy.md
#: 333303d5e7054d2697f11bb2b53e92ec
msgid "Smooth conversation inference, better than V100"
msgstr "丝滑的对话体验，性能好于V100"

#: ../../getting_started/install/deploy/deploy.md
#: acfb9e152ec74bacb715cd758a9be964
msgid "V100"
msgstr "V100"

#: ../../getting_started/install/deploy/deploy.md
#: 1127ebc45a1b4fdb828f13d55f99ce79 ed6dd464a7f640cd866712eb7f4d4b1b
msgid "16 GB"
msgstr "16 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 0b6ef92756204be6984f39ee1b95d423 a5aad9380cd742c59934e5ead433a22e
msgid "Conversation inference possible, noticeable stutter"
msgstr "Conversation inference possible, noticeable stutter"

#: ../../getting_started/install/deploy/deploy.md
#: 7a5a394f02b04cdcb3a9785cfa0cfc7c
msgid "T4"
msgstr "T4"

#: ../../getting_started/install/deploy/deploy.md:30
#: 055b00b30901485a841d958a63750341
#, fuzzy
msgid ""
"If your VRAM Size is not enough, DB-GPT supported 8-bit quantization and "
"4-bit quantization."
msgstr "如果你的显存不够，DB-GPT支持8-bit和4-bit量化版本"

#: ../../getting_started/install/deploy/deploy.md:32
#: 34175af61e2e4ecc9e56180b8872a30b
msgid ""
"Here are some of the VRAM size usage of the models we tested in some "
"common scenarios."
msgstr "这里是量化版本的相关说明"

#: ../../getting_started/install/deploy/deploy.md
#: 1d1adf0f341b4ed7b47887c5923fbe08
msgid "Model"
msgstr "Model"

#: ../../getting_started/install/deploy/deploy.md
#: 2e36f6ffdb084de3ae3d1568af60cecc
msgid "Quantize"
msgstr "Quantize"

#: ../../getting_started/install/deploy/deploy.md
#: 54df9f9813274db482a683c001003e86 61380cdbc434467fbf9cb7cb1efe49b7
msgid "vicuna-7b-v1.5"
msgstr "vicuna-7b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 4390ae926c094187bf2905361a5d6cff 467d31fb940c4daf9b2afec6bb7ea7f0
#: 525896b27182457486018a348c068c01 6788cb202dc044e59e6ae42936b1aca8
#: 74e07aaf7fa7461e824c653129240ad1 83b491c17b434fdb910b92c4cbc007a0
#: c14cc4d0ac384fc699a196bf62573f01
msgid "4-bit"
msgstr "4-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 25bbb9742e604003aeb2da782e50fa46 334153c71b624064b4f50342ab79c30e
#: 6039634dfcfc4ad3a3298938534ef1e4
msgid "8 GB"
msgstr "8 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 0d31a4a235b949eabd8e98c2dcb6d5ff 15bcdb7aedf1497fb0790c1bf3e5ee47
#: 3125bc143cb0476db0a07b7788bc9928 be257ed0772d448b95873db9e044a713
#: cf4e3ed197a84dba87a60cc5fc70f8ac eb399647bafe418c90212787e695afbb
#: ee40d24463a143ca8768da7423d25b9b
msgid "8-bit"
msgstr "8-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 21efb501692440cf80fd29401e1f0afa 246e4fc9b5f44f42a36bb49fd65c08f2
#: 9adc65ad9d9344efa83f3507fb6ed2fd b00fd0d15bf441f48f9ea75d8877d4fd
#: d92aa46491b442a69709b9b8d7322c2e f8499e166ca04bc2a84bfa2d42cee890
msgid "12 GB"
msgstr "12 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 39950e71d2884d2c8ce4dbc9b0cb4491 ab3349160edf447ea67b15d7a056cc6e
msgid "vicuna-13b-v1.5"
msgstr "vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 53bd397b40e449988d9fdfd201030387 6123ea3af97f4f3dafc6be44ecfed416
#: f493c256788e4a318cf57fa9340948a4
msgid "20 GB"
msgstr "20 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 11903db1ac944b60a40c92f752c1f3dc 76831b0fad014348a081f3f64260c73e
msgid "llama-2-7b"
msgstr "llama-2-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 2576bd18e1714557b33420e5ed56a95b 997fe3a3a46e4891b39171b81386a601
msgid "llama-2-13b"
msgstr "llama-2-13b"

#: ../../getting_started/install/deploy/deploy.md
#: 8a1cf1ae302c4c2d8bfbb1a666cc9ba6 9cbe2dae8bf44181b24d9806b654b80f
msgid "llama-2-70b"
msgstr "llama-2-70b"

#: ../../getting_started/install/deploy/deploy.md
#: d5e875e84f534aac8f2cac4eacde7ead
msgid "48 GB"
msgstr "48 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 2e32ea59cce24ff0a96c8c152afeb09b
msgid "80 GB"
msgstr "80 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 176feab13e554986a1e09ce3c1d060ee edcf6676d3274fd3a578d337894467bf
msgid "baichuan-7b"
msgstr "baichuan-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 3da976a65887483abc029acb7e7640d4 837745bfb1ac41a99604f55e94fd4099
msgid "baichuan-13b"
msgstr "baichuan-13b"

#: ../../getting_started/install/deploy/deploy.md:51
#: 3c573a548e6a4767b4acc2e4d2dbd20c
msgid "2. Install"
msgstr "2. Install"

#: ../../getting_started/install/deploy/deploy.md:56
#: 40d61f4ee30d4d70a45a0ffb97001cd6
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration.  For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. [How to install "
"Miniconda](https://docs.conda.io/en/latest/miniconda.html)"
msgstr ""
"目前使用Sqlite作为默认数据库，因此DB-"
"GPT快速部署不需要部署相关数据库服务。如果你想使用其他数据库，需要先部署相关数据库服务。我们目前使用Miniconda进行python环境和包依赖管理[安装"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy/deploy.md:67
#: 69c950c69b204b94a768d1f023cc978a
msgid ""
"Once the environment is installed, we have to create a new folder "
"\"models\" in the DB-GPT project, and then we can put all the models "
"downloaded from huggingface in this directory"
msgstr "如果你已经安装好了环境需要创建models, 然后到huggingface官网下载模型"

#: ../../getting_started/install/deploy/deploy.md:70
#: 2ab1a4d9de8e412f80bd04ce6b40cdf6
msgid "Notice make sure you have install git-lfs"
msgstr "注意确认你已经安装了git-lfs"

#: ../../getting_started/install/deploy/deploy.md:72
#: e718db01f5404485a05857b8403df93c
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:74
#: e98570774d28430295a094cc5f5220ae
msgid "ubuntu:apt-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:76
#: 34c33b64b8de4b00be92b525ad038f23
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:78
#: 34bd5fbc8a8c4898a4caa7d630137061
msgid "Download LLM Model and Embedding Model"
msgstr "下载LLM模型和Embedding模型"

#: ../../getting_started/install/deploy/deploy.md:80
#: a67d0e365e684a3bbb5e8618c98884ce
#, fuzzy
msgid ""
"If you use OpenAI llm service, see [How to Use LLM REST API](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/llm/proxyllm/proxyllm.html)"
msgstr "如果想使用openai大模型服务, 可以参考[如何集成LLM REST API](https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-cn/zh-cn/latest/getting_started/install/llm/proxyllm/proxyllm.html)"

#: ../../getting_started/install/deploy/deploy.md:83
#: f1a43cd2eba3458c863bfc77cf13ac1f
#, fuzzy
msgid ""
"If you use openai or Axzure or tongyi llm api service, you don't need to "
"download llm model."
msgstr "如果你想通过openai or Azure or tongyi第三方api访问模型服务，你可以不用下载llm模型"

#: ../../getting_started/install/deploy/deploy.md:103
#: 9f46746726ec4791b6963a2e2c4376c4
msgid ""
"The model files are large and will take a long time to download. During "
"the download, let's configure the .env file, which needs to be copied and"
" created from the .env.template"
msgstr "模型文件很大，需要很长时间才能下载。在下载过程中，让我们配置.env文件，它需要从。env.template中复制和创建。"

#: ../../getting_started/install/deploy/deploy.md:106
#: 53e713c8a9664d92a6e4055789c4a7da
msgid "cp .env.template .env"
msgstr "cp .env.template .env"

#: ../../getting_started/install/deploy/deploy.md:109
#: 21c38ebc721242478e7ad4be4d672dc6
msgid ""
"You can configure basic parameters in the .env file, for example setting "
"LLM_MODEL to the model to be used"
msgstr "您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。"

#: ../../getting_started/install/deploy/deploy.md:111
#: 6ca180c2142b455ebcc3a8b37f3bc25a
msgid ""
"([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。([Vicuna-v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5)， "
"目前Vicuna-v1.5模型(基于llama2)已经开源了，我们推荐你使用这个模型通过设置LLM_MODEL=vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md:113
#: 1a346658ec4b4074b3458fc806538aae
msgid "3. Run"
msgstr "3. Run"

#: ../../getting_started/install/deploy/deploy.md:115
#: 70f6300673834c9eb3e80145bb5bfcb8
msgid "**(Optional) load examples into SQLite**"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:120
#: c10f16bd300e473a950617b814d743a0
msgid "On windows platform:"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:125
#: 64213616228643a7b0413805061b7a12
#, fuzzy
msgid "Run db-gpt server"
msgstr "1.Run db-gpt server"

#: ../../getting_started/install/deploy/deploy.md:131
#: cb3248ebbade45bfba1366a66e4220f6
msgid "Open http://localhost:5000 with your browser to see the product."
msgstr "打开浏览器访问http://localhost:5000"

#: ../../getting_started/install/deploy/deploy.md:134
#: 0499acfd344b4d70a0cdce31f245971c
#, fuzzy
msgid "Multiple GPUs"
msgstr "4. Multiple GPUs"

#: ../../getting_started/install/deploy/deploy.md:136
#: d56b6c4aa795428aa64e3740401645d3
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPT默认加载可利用的gpu，你也可以通过修改 在`.env`文件 `CUDA_VISIBLE_DEVICES=0,1`来指定gpu IDs"

#: ../../getting_started/install/deploy/deploy.md:138
#: ca208283904441ab802827d94b3b9590
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "你也可以指定gpu ID启动"

#: ../../getting_started/install/deploy/deploy.md:148
#: 30f7c3b3d9784a4698cdd15a0c046e81
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "同时你可以通过在.env文件设置`MAX_GPU_MEMORY=xxGib`修改每个GPU的最大使用内存"

#: ../../getting_started/install/deploy/deploy.md:150
#: ab02884bb7f24e59b21b797501c3794b
#, fuzzy
msgid "Not Enough Memory"
msgstr "5. Not Enough Memory"

#: ../../getting_started/install/deploy/deploy.md:152
#: 5b246b0456f8448bb0207312a17d40c5
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT 支持 8-bit quantization 和 4-bit quantization."

#: ../../getting_started/install/deploy/deploy.md:154
#: a002dc2572d34b0f9b3ca6ac3b3b6147
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "你可以通过在.env文件设置`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/install/deploy/deploy.md:156
#: 16d429bd42a44b02875e505273d35228
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization 可以运行在80GB VRAM机器， 4-bit "
"quantization可以运行在 48 GB  VRAM"

#~ msgid ""
#~ "Notice make sure you have install "
#~ "git-lfs centos:yum install git-lfs "
#~ "ubuntu:app-get install git-lfs "
#~ "macos:brew install git-lfs"
#~ msgstr ""
#~ "注意下载模型之前确保git-lfs已经安ubuntu:app-get install "
#~ "git-lfs macos:brew install git-lfs"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "你可以参考如何获取Vicuna weights文档[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) ."

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "如果觉得模型太大你也可以下载vicuna-7b [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) "

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr ""
#~ "如果你想访问外部的大模型服务(是通过DB-"
#~ "GPT/pilot/server/llmserver.py启动的模型服务)，1.需要在.env文件设置模型名和外部模型服务地址。2.使用light模式启动服务"

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""
#~ "注意，需要安装[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)涉及的所有的依赖"

#~ msgid "ubuntu:app-get install git-lfs"
#~ msgstr ""

#~ msgid "Before use DB-GPT Knowledge"
#~ msgstr "在使用知识库之前"

#~ msgid "**(Optional) load examples into SQLlite**"
#~ msgstr ""

#~ msgid "If you want to access an external LLM service, you need to"
#~ msgstr ""

#~ msgid ""
#~ "1.set the variables LLM_MODEL=YOUR_MODEL_NAME, "
#~ "MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in "
#~ "the .env file."
#~ msgstr ""

#~ msgid "2.execute dbgpt_server.py in light mode"
#~ msgstr ""

#~ msgid ""
#~ "If you want to learn about "
#~ "dbgpt-webui, read https://github./csunny/DB-"
#~ "GPT/tree/new-page-framework/datacenter"
#~ msgstr ""
#~ "如果你想了解web-ui, 请访问https://github./csunny/DB-GPT/tree"
#~ "/new-page-framework/datacenter"

